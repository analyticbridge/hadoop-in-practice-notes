# comma-separated brokers in "host:port" format
kafka.brokers=localhost:9092

# Name of the client as seen by kafka
kafka.client.name=hip

# Top-level data output directory, sub-directories will be dynamically created for each topic pulled
etl.destination.path=/tmp/camus/dest

# HDFS location where you want to keep execution files, i.e. offsets, error logs, and count files
etl.execution.base.path=/tmp/camus/work

# Where completed Camus job output directories are kept, usually a sub-dir in the base.path
etl.execution.history.path=/tmp/camus/history

#camus.message.decoder.class=com.linkedin.camus.etl.kafka.coders.KafkaAvroMessageDecoder
camus.message.decoder.class=hip.ch5.kafka.camus.StockMessageDecoder
etl.record.writer.provider.class=com.linkedin.camus.etl.kafka.common.AvroRecordWriterProvider
kafka.message.coder.schema.registry.class=hip.ch5.kafka.camus.StockSchemaRegistry

# All files in this dir will be added to the distributed cache and placed on the classpath for hadoop tasks
# hdfs.default.classpath.dir=

# Max hadoop tasks to use, each task can pull multiple topic partitions
mapred.map.tasks=2

# Max historical time that will be pulled from each partition based on event timestamp
# kafka.max.pull.hrs=1

# Events with a timestamp older than this will be discarded.
# kafka.max.historical.days=3

# Max minutes for each mapper to pull messages
# kafka.max.pull.minutes.per.task=-1

# Decoder class for Kafka Messages to Avro Records
# camus.message.decoder.class=

# If whitelist has values, only whitelisted topic are pulled. Nothing on the blacklist is pulled
# kafka.blacklist.topics=
# kafka.whitelist.topics=
